<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Yin Guangqi</title>
    <link>https://xjtu-ygq.github.io/posts/</link>
    <description>Recent content in Posts on Yin Guangqi</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 27 Nov 2023 21:52:48 +0800</lastBuildDate><atom:link href="https://xjtu-ygq.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>20231127--尹光起！生日快乐！</title>
      <link>https://xjtu-ygq.github.io/posts/20231127--%E5%B0%B9%E5%85%89%E8%B5%B7%E7%94%9F%E6%97%A5%E5%BF%AB%E4%B9%90/</link>
      <pubDate>Mon, 27 Nov 2023 21:52:48 +0800</pubDate>
      
      <guid>https://xjtu-ygq.github.io/posts/20231127--%E5%B0%B9%E5%85%89%E8%B5%B7%E7%94%9F%E6%97%A5%E5%BF%AB%E4%B9%90/</guid>
      <description>接口测试</description>
    </item>
    
    <item>
      <title>优化器</title>
      <link>https://xjtu-ygq.github.io/posts/%E4%BC%98%E5%8C%96%E5%99%A8/</link>
      <pubDate>Tue, 21 Mar 2023 17:44:00 +0800</pubDate>
      
      <guid>https://xjtu-ygq.github.io/posts/%E4%BC%98%E5%8C%96%E5%99%A8/</guid>
      <description>优化器 ​ 优化器就是在深度学习反向传播过程中，指引损失函数（目标函数）的各个参数往正确的方向更新合适的大小，使得更新后的各个参数让损失函数（目标函数）值不断逼近全局最小 SGD（随机梯度下降） $$ w_{t+1}=w_t-\alpha\cdot g_t $$ 每次</description>
    </item>
    
    <item>
      <title>注意事项</title>
      <link>https://xjtu-ygq.github.io/posts/%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/</link>
      <pubDate>Tue, 21 Mar 2023 15:31:56 +0800</pubDate>
      
      <guid>https://xjtu-ygq.github.io/posts/%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/</guid>
      <description>Transformer Transformer的多头注意力机制为什么每个头计算结果不同？ 输入相同，但学习的投影矩阵（linear）不同，原因在于投影矩阵的初始值是随机初始化的，所以学习结果不用 为什么Q和K使用不同的权重矩阵</description>
    </item>
    
    <item>
      <title>DeepCTR模型框架</title>
      <link>https://xjtu-ygq.github.io/posts/deepctr%E6%A8%A1%E5%9E%8B%E6%A1%86%E6%9E%B6/</link>
      <pubDate>Wed, 01 Mar 2023 14:38:33 +0800</pubDate>
      
      <guid>https://xjtu-ygq.github.io/posts/deepctr%E6%A8%A1%E5%9E%8B%E6%A1%86%E6%9E%B6/</guid>
      <description>参考：torch-rechub 源码：DeepCTR Wide&amp;amp;Deep、DeepFM、DCN 数据集：Criteo 注意： 用于实际项目时，主要修改data.py文件，进行数据处理 数据处理中使用了La</description>
    </item>
    
    <item>
      <title>LR公式推导</title>
      <link>https://xjtu-ygq.github.io/posts/lr%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/</link>
      <pubDate>Thu, 02 Feb 2023 09:47:27 +0800</pubDate>
      
      <guid>https://xjtu-ygq.github.io/posts/lr%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/</guid>
      <description>线性回归公式： $$ y=w^Tx+b $$ 简化线性回归公式： $$ y=w^Tx $$ sigmoid公式： $$ \sigma(x)=\frac{1}{1+e^{-x}} $$ LR公式： $$ \pi(x)=\frac{1}{1+e^{-w^Tx}} $$ 二分类： $$ P(y=1|x)=\frac{1}{1+e^{-w^Tx}} $$ $$ \begin{equation} \begin{aligned} P(y=0|x)&amp;amp;=1-P(y=1|x)\\ &amp;amp;=\frac{e^{-w^Tx}}{1+e^{-w^Tx}} \end{aligned} \end{equation} $$ 对数似然函数： $$ \begin{equation} \begin{aligned} L(x)&amp;amp;= \prod\limits^N_{i=1}logP(y_i|x_i)\\ &amp;amp;=\sum\limits^N_{i=1}(y_ilog(p_1)+(1-y_ilog(p_0)))\\ &amp;amp;=\sum\limits^N_{i=1}(y_ilog(p_1)+(1-y_ilog(1-p_1)))\\ &amp;amp;=\sum\limits^N_{i=1}(y_i(log(p_1)-log(1-p_1))+log(1-p_1))\\ &amp;amp;=\sum\limits^N_{i=1}(y_ilog\frac{p_1}{1-p_1}+log(1-p_1))\\ &amp;amp;=\sum\limits^N_{i=1}(y_iw^Tx_i-log(1+e^{w^Tx})) \end{aligned} \end{equation} $$ 求偏导： $$ \begin{equation} \begin{aligned} \frac{\partial L(x)}{\partial w}&amp;amp;=\sum\limits^N_{i=1}(y_ix_i-\frac{x_ie^{w^Tx_i}}{1+e^{w^Tx_i}})\\ &amp;amp;=\sum\limits^N_{i=1}(y_i-\frac{e^{w^Tx_i}}{1+e^{w^Tx_i}})x_i\\ &amp;amp;=\sum\limits^N_{i=1}(y_i-\frac{1}{1+e^{-w^Tx_i}})x_i\\ &amp;amp;=\sum\limits^N_{i=1}(y_i-\pi(w^Tx))x_i\\ \end{aligned} \end{equation} $$ 梯度上升： $$</description>
    </item>
    
    <item>
      <title>模拟面试</title>
      <link>https://xjtu-ygq.github.io/posts/%E6%A8%A1%E6%8B%9F%E9%9D%A2%E8%AF%95/</link>
      <pubDate>Wed, 28 Dec 2022 20:02:12 +0800</pubDate>
      
      <guid>https://xjtu-ygq.github.io/posts/%E6%A8%A1%E6%8B%9F%E9%9D%A2%E8%AF%95/</guid>
      <description>快手 两数相除 class Solution { public int divide(int dividend, int divisor) { int res=0; if(dividend==Integer.MIN_VALUE&amp;amp;&amp;amp;divisor==-1) return Integer.MAX_VALUE; int sign=(dividend&amp;gt;0)^(divisor&amp;gt;0)?-1:1; dividend=Math.abs(dividend); divisor=Math.abs(divisor); for(int i=31;i&amp;gt;=0;i--){ if((dividend&amp;gt;&amp;gt;&amp;gt;i)-divisor&amp;gt;=0){ dividend-=(divisor&amp;lt;&amp;lt;i); res+=(1&amp;lt;&amp;lt;i); } } return sign==1?res:-res; } } 复原 IP 地址 class Solution { public int divide(int dividend, int divisor) { int res=0; if(dividend==Integer.MIN_VALUE&amp;amp;&amp;amp;divisor==-1) return Integer.MAX_VALUE; int sign=(dividend&amp;gt;0)^(divisor&amp;gt;0)?-1:1; dividend=Math.abs(dividend); divisor=Math.abs(divisor); for(int i=31;i&amp;gt;=0;i--){ if((dividend&amp;gt;&amp;gt;&amp;gt;i)-divisor&amp;gt;=0){ dividend-=(divisor&amp;lt;&amp;lt;i); res+=(1&amp;lt;&amp;lt;i); } } return sign==1?res:-res; } } 24 点游戏 private static final double target=24; private static final double epsilon=1e-6; public boolean judgePoint24(int[] cards) { return helper(new double[]{cards[0],cards[1],cards[2],cards[3]}); } public boolean helper(double[] nums){ if(nums.length==1){ return</description>
    </item>
    
    <item>
      <title>YouTubeDNN</title>
      <link>https://xjtu-ygq.github.io/posts/youtubednn/</link>
      <pubDate>Sat, 10 Dec 2022 20:47:21 +0800</pubDate>
      
      <guid>https://xjtu-ygq.github.io/posts/youtubednn/</guid>
      <description>推荐系统架构 Candidate Generation Model Ranking Model 常见问题 文中把推荐问题转换成多分类问题，在预测next watch的场景下，每一个备选video都会是一个分类，因此总共的分类有数百万之巨，这在使用softmax训练时无疑是低效的</description>
    </item>
    
    <item>
      <title>Transformer的Pytorch实现</title>
      <link>https://xjtu-ygq.github.io/posts/transformer%E7%9A%84pytorch%E5%AE%9E%E7%8E%B0/</link>
      <pubDate>Fri, 02 Dec 2022 21:48:16 +0800</pubDate>
      
      <guid>https://xjtu-ygq.github.io/posts/transformer%E7%9A%84pytorch%E5%AE%9E%E7%8E%B0/</guid>
      <description>写在前面 Transformer主要分为图示12块，详细介绍请参考： The Annotated Transformer The Annotated Transformer的中文注释版 前置知识：切记了解pytorch的class类使用规则 class定义-&amp;gt;对象-&amp;gt</description>
    </item>
    
    <item>
      <title>Attention Is All You Need</title>
      <link>https://xjtu-ygq.github.io/posts/transformer/</link>
      <pubDate>Thu, 01 Dec 2022 18:53:25 +0800</pubDate>
      
      <guid>https://xjtu-ygq.github.io/posts/transformer/</guid>
      <description>Encoder+Decoder Outputs是自己的输出，只在训练时加入，应用时不加入 $sub-layer=LayerNorm(x+SubLayer(x))$ Encoder的输出作为Decoder多头注意力机制的Q、K 激活函数采用Relu Attention 多头注意力机制是将Q、K、V进行全连接之后再进行h层的注意</description>
    </item>
    
    <item>
      <title>集成学习</title>
      <link>https://xjtu-ygq.github.io/posts/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/</link>
      <pubDate>Wed, 30 Nov 2022 21:31:29 +0800</pubDate>
      
      <guid>https://xjtu-ygq.github.io/posts/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/</guid>
      <description>Bagging 自主采样成多分类器 多个学习器并行训练，互不干扰，少数服从多数（投票表决） 随机森林 随机 随机样本 随机特征 森林 多个树并行，互不依赖 参考：什么是随机森林 Boosting 将弱学习器提升为强学习器 GBDT G：梯度 B：提升（boos</description>
    </item>
    
    <item>
      <title>支持向量机</title>
      <link>https://xjtu-ygq.github.io/posts/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</link>
      <pubDate>Wed, 30 Nov 2022 16:52:31 +0800</pubDate>
      
      <guid>https://xjtu-ygq.github.io/posts/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</guid>
      <description>线性可分支持向量机 线性分类器$g(x)=w^Tx+w_0$存在分类面$g(x)=0$满足： 分类面与两类样本间距离相等 分类面与两类样本间距离最大 支持向量（狭义）：距离分类面最近的那几个点 线性鉴别函数与分</description>
    </item>
    
    <item>
      <title>《统计学习方法》与《机器学习》</title>
      <link>https://xjtu-ygq.github.io/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
      <pubDate>Tue, 29 Nov 2022 19:39:00 +0800</pubDate>
      
      <guid>https://xjtu-ygq.github.io/posts/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</guid>
      <description>性能度量 混淆矩阵 真实情况 预测结果 正例 反例 正例 TP（真正例） FN（假反例） 反例 FP（假正例） TN（真反例） 偏差与方差 偏差：期望输出与真实标记的差别 $$ bias^2=(\bar f(x)-y)^2 $$ 方差：由样本数相同的不同训练集产生 $$ var(x)=E_D[(f(x;D)-\bar f(x))^2] $$ 噪声 $$</description>
    </item>
    
    <item>
      <title>Alibaba推荐算法模型</title>
      <link>https://xjtu-ygq.github.io/posts/alibaba%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Tue, 29 Nov 2022 14:56:03 +0800</pubDate>
      
      <guid>https://xjtu-ygq.github.io/posts/alibaba%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E6%A8%A1%E5%9E%8B/</guid>
      <description>DIN、DIEN、BST模型 DIN模型 特点： 挖掘用户历史行为（兴趣分布） Attention机制实现Local Activation Dice激活函数、自适应正则化防止过拟合（数据稀疏） Attention 输入：用户历史行为的物品Embedd</description>
    </item>
    
    <item>
      <title>《深度学习推荐系统》笔记</title>
      <link>https://xjtu-ygq.github.io/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Mon, 28 Nov 2022 16:19:03 +0800</pubDate>
      
      <guid>https://xjtu-ygq.github.io/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%AC%94%E8%AE%B0/</guid>
      <description>深度学习推荐系统 传统推荐模型 协同过滤CF 协同过滤的核心是向量的相似度Similarity 余弦相似度 皮尔逊相关系数 用户平均分 物品平均分 UserCF 用户的评分向量（横向量）的相似度，计算相似用户TopN，目标评分=</description>
    </item>
    
  </channel>
</rss>
