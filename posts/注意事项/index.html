<!DOCTYPE html>
<html lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="VarKai" />
    <meta name="referrer" content="never">
	
	
	
	<title>注意事项 ｜ Yin Guangqi</title>
	
    
    
    <meta name="description" content="Transformer Transformer的多头注意力机制为什么每个头计算结果不同？ 输入相同，但学习的投影矩阵（linear）不同，原因在于投影矩阵的初始值是随机初始化的，所以学习结果不用 为什么Q和K使用不同的权重矩阵" />
    

    
    
    <meta name="keywords" content="Transformer" />
    

	
    
    <link rel="shortcut icon" href="https://xjtu-ygq.github.io/images/favicon.ico" />

    <link rel="stylesheet" type="text/css" media="screen" href="https://xjtu-ygq.github.io/css/normalize.css" />
    <link rel="stylesheet" type="text/css" media="screen" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.0/animate.min.css" />
    <link rel="stylesheet" type="text/css" media="screen" href="https://xjtu-ygq.github.io/css/zozo.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" />
    <link rel="stylesheet" type="text/css" media="screen" href="https://xjtu-ygq.github.io/css/highlight.css" />

    
    
</head>

<body>
    <div class="main animate__animated animate__fadeInDown">
        <div class="nav_container animated fadeInDown">
    <div class="site_nav" id="site_nav">
        <ul>
            
            <li>
                <a href="/">Home</a>
            </li>
            
            <li>
                <a href="/posts/">Archive</a>
            </li>
            
            <li>
                <a href="/tags/">Tags</a>
            </li>
            
            <li>
                <a href="/about/">About</a>
            </li>
            
        </ul>
    </div>
    <div class="menu_icon">
        <a id="menu_icon"><i class="ri-menu-line"></i></a>
    </div>
</div>
        <div class="header animated fadeInDown">
    <div class="site_title_container">
        <div class="site_title">
            <h1>
                <a href="https://xjtu-ygq.github.io/">
                    <span>Yin Guangqi</span>
                </a>
            </h1>
        </div>
        <div class="description">
            <p class="sub_title">一日之计在于晨</p>
            <div class="my_socials">
                
                
                <a href="%20" title="facebook" target="_blank"><i class="ri-facebook-fill"></i></a>
                
                
                
                <a href="https://github.com/xjtu-ygq" title="github" target="_blank"><i class="ri-github-fill"></i></a>
                
                
                
                <a href="%20" title="instagram" target="_blank"><i class="ri-instagram-fill"></i></a>
                
                
                
                <a href="https://twitter.com/QQ32267010" title="twitter" target="_blank"><i class="ri-twitter-fill"></i></a>
                
                
                
                <a href="https://www.weibo.com/u/6612072892" title="weibo" target="_blank"><i class="ri-weibo-fill"></i></a>
                
                
                <a href="https://xjtu-ygq.github.io/index.xml" type="application/rss+xml" title="rss" target="_blank"><i
                        class="ri-rss-fill"></i></a>
            </div>
        </div>
    </div>
</div>
        <div class="content">
            <div class="post_page">
                <div class="post animate__animated animate__fadeInDown">
                    <div class="post_title post_detail_title">
                        <h2><a href='/posts/%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/'>注意事项</a></h2>
                        <span class="date">2023.03.21</span>
                    </div>
                    <div class="post_content markdown"><h2 id="transformer">Transformer</h2>
<ol>
<li>Transformer的多头注意力机制为什么每个头计算结果不同？
<ul>
<li>输入相同，但学习的投影矩阵（linear）不同，原因在于投影矩阵的初始值是<strong>随机初始化</strong>的，所以学习结果不用</li>
</ul>
</li>
<li>为什么Q和K使用不同的权重矩阵生成？（权重矩阵是指全连接处）
<ul>
<li>Q、K点乘需要维度一致（nxm和mxd），所以<strong>序列的长度</strong>不同导权重矩阵不同</li>
<li>注意力机制中，Q、K经过<strong>全连接层</strong> ，不同的权重矩阵有利于捕捉到更加充分的潜在信息</li>
</ul>
</li>
<li>注意力机制为何采用点乘而不是加法？
<ul>
<li>点乘代表向量相似度，加法只是简单想法，不具有可解释性</li>
<li>二者复杂度相同，效果相似，具体以实践为主</li>
</ul>
</li>
<li>attention之后需要进行<strong>softmax</strong>，解释为何要scaled？
<ul>
<li>scaled即除以根号d，防止softma饱和问题（<strong>受到单一值影响过大</strong>）</li>
<li>提高模型的训练和预测性能（速度）</li>
</ul>
</li>
<li>Transformer的掩码机制：
<ul>
<li>掩码只在<strong>解码器decoder</strong>中</li>
<li>在输入的时候设置mask标记哪些是padding或是当前时刻之后的数据（比如0,1）</li>
<li>主要体现在<strong>attention计算</strong>中，在<strong>softmax</strong>之前赋值负无穷，使得softmax之后变为0</li>
</ul>
</li>
<li>为什么在进行多头注意力的时候需要对每个head进行降维？
<ul>
<li>这个降维体现在<strong>linear</strong>或投影矩阵</li>
<li>降低计算复杂度（主要考虑序列长度问题）</li>
<li>效果和速度之间抉择（<strong>注意是linear降维还是softmax之前的除以根号d</strong>）</li>
</ul>
</li>
<li>Encoder介绍：
<ul>
<li>两层：多头注意力机制+前馈神经网络</li>
<li>嵌入层、位置编码、注意力机制（残差、归一化)、前馈神经网络（残差、归一化）</li>
</ul>
</li>
<li>Transformer哪些地方使用了激活函数
<ul>
<li>前馈神经网络（全连接+Relu）</li>
<li>多头注意力机制（attention之后的softmax）</li>
</ul>
</li>
<li>为什么transformer块使用LayerNorm而不是BatchNorm？LayerNorm 在Transformer的位置是哪里？</li>
<li>前馈神经网络：
<ul>
<li>两层全连接（<strong>linear+Relu+linear</strong>）</li>
<li>Relu激活函数计算速度快，增加非线性表达能力</li>
<li>FFN不能处理元素之间的关系（因为只有linear）</li>
</ul>
</li>
<li>Dropout：
<ul>
<li>随机断开神经元之间的连接</li>
<li>防止过拟合</li>
</ul>
</li>
</ol>
<h2 id="其他">其他</h2>
<ol>
<li>
<p>负样本：</p>
<ul>
<li>
<p>常见方法：</p>
<ul>
<li>
<p>曝光未点击</p>
</li>
<li>
<p>随机负采样：全样本去除正样本，随机采样</p>
</li>
<li>
<p>流行负样本：流行样本但未点击（流行样本都没有点击，说明是不喜欢）</p>
</li>
</ul>
</li>
<li>
<p>召回阶段与排序阶段：</p>
<ul>
<li>召回：随机负采样</li>
<li>排序：曝光未点击</li>
</ul>
</li>
</ul>
</li>
<li></li>
</ol>
</div>
                    <div class="post_footer">
                        
                        <div class="meta">
                            <div class="info">
                                <span class="field tags">
                                    <i class="ri-stack-line"></i>
                                    
                                    <a href="https://xjtu-ygq.github.io/tags/transformer/">Transformer</a>
                                    
                                </span>
                            </div>
                        </div>
                        
                    </div>
                </div>
                
                
                <div class="doc_comments"></div>
                
            </div>
        </div>
    </div>
    <a id="back_to_top" href="#" class="back_to_top"><i class="ri-arrow-up-s-line"></i></a>
    <footer class="footer">
    <div class="powered_by">
        <a href="http://github.com/xjtu-ygq">© xjtu-ygq | </a>
     
        <a href="http://www.gohugo.io/">Powered by Hugo | </a>
        <a href="https://zozo.varkai.com/">Theme - varkai | </a>
        
    </div>

    
    
</footer>
    <script src="https://xjtu-ygq.github.io/js/jquery-3.5.1.min.js"></script>
<link href="https://xjtu-ygq.github.io/css/fancybox.min.css" rel="stylesheet">
<script src="https://xjtu-ygq.github.io/js/fancybox.min.js"></script>
<script src="https://xjtu-ygq.github.io/js/zozo.js"></script>


<script type="text/javascript" async
    src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\[\[', '\]\]']],
                processEscapes: true,
                processEnvironments: true,
                skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
                TeX: {
                    equationNumbers: { autoNumber: "AMS" },
                    extensions: ["AMSmath.js", "AMSsymbols.js"]
                }
            }
        });

        MathJax.Hub.Queue(function () {
            
            
            
            var all = MathJax.Hub.getAllJax(), i;
            for (i = 0; i < all.length; i += 1) {
                all[i].SourceElement().parentNode.className += ' has-jax';
            }
        });
    </script>

<style>
    code.has-jax {
        font: inherit;
        font-size: 100%;
        background: inherit;
        border: inherit;
        color: #515151;
    }
</style>



</body>

</html>